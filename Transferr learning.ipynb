{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classfier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "def face_extractor(img):\n",
    "    faces = face_classfier.detectMultiScale(img,1.3,5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    for(x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h,x:x+w]\n",
    "        \n",
    "    return cropped_face\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame),(200,200))\n",
    "        file_name_path = 'E://VM RHEL 8 C2//AWORKSPACE//nik//'+ str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path,face)\n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "    if cv2.waitKey(1) == 13 or count == 200:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Sample collection complete !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0InputLayer False\n",
      "1Conv2D False\n",
      "2Conv2D False\n",
      "3MaxPooling2D False\n",
      "4Conv2D False\n",
      "5Conv2D False\n",
      "6MaxPooling2D False\n",
      "7Conv2D False\n",
      "8Conv2D False\n",
      "9Conv2D False\n",
      "10MaxPooling2D False\n",
      "11Conv2D False\n",
      "12Conv2D False\n",
      "13Conv2D False\n",
      "14MaxPooling2D False\n",
      "15Conv2D False\n",
      "16Conv2D False\n",
      "17Conv2D False\n",
      "18MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "rows = 224\n",
    "cols = 224\n",
    "model = VGG16(weights = 'imagenet' , include_top=False, input_shape= (rows,cols,3))\n",
    "model.save('face_recog_vgg.h5')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \"\" + layer.__class__.__name__,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlayer(bottom_model , num_classes):\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(1024,activation='relu')(top_model)\n",
    "    top_model = Dense(1024,activation='relu')(top_model)    \n",
    "    top_model = Dense(512,activation='relu')(top_model)    \n",
    "    top_model = Dense(num_classes,activation='softmax')(top_model)  \n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x219bb2cf408>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c41a6ec8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c41c1c08>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x219c431ec88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c431efc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c432ae08>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x219c4331f08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c432af08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c433ce48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c4554508>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x219c4558788>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c4558048>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c45646c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c456ef48>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x219c4575ac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c4575888>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c457db08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x219c457f2c8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x219c458a788>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 16,815,426\n",
      "Trainable params: 2,100,738\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "num_classes = 2\n",
    "FC_Head = addlayer(model,num_classes)\n",
    "modelnew=Model(inputs=model.input,outputs=FC_Head)\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 475 images belonging to 2 classes.\n",
      "Found 11 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "train_data_dir = 'dataset/train/'\n",
    "validation_data_dir = 'dataset/test/'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    "train_generator =train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=(rows,cols),\n",
    "                                                  batch_size=train_batchsize,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  )\n",
    "validation_generator =validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                  target_size=(rows,cols),\n",
    "                                                  batch_size=val_batchsize,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "74/74 [==============================] - 488s 7s/step - loss: 0.3145 - accuracy: 0.8999 - val_loss: 3.3614 - val_accuracy: 0.7273\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.36136, saving model to face_recog_vgg.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "checkpoint=ModelCheckpoint(\"face_recog_vgg.h5\",monitor=\"val_loss\",mode=\"min\", save_best_only = True,verbose=1)\n",
    "earlystop=EarlyStopping(monitor='val_loss',min_delta = 0, patience=3,verbose=1,restore_best_weights=True)\n",
    "callbacks=[earlystop,checkpoint]\n",
    "\n",
    "modelnew.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001),metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples=1190\n",
    "nb_validation_samples=170\n",
    "epoches=1\n",
    "batch_size=16\n",
    "\n",
    "history = modelnew.fit_generator(train_generator,steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                epochs=epoches,callbacks=callbacks,\n",
    "                                validation_data = validation_generator,\n",
    "                                validation_steps=nb_validation_samples // batch_size)\n",
    "modelnew.save(\"face_recog_vgg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "classifier = load_model('face_recog_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_dict = {\"[0]\":\"nik\",\"[1]\":\"yash\"}\n",
    "person_dict_n = {\"nik\":\"nik\",\"yash\":\"yash\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_test(name,pred,im):\n",
    "    person=person_dict[str(pred)]\n",
    "    BLACK=[0,0,0]\n",
    "    expanded_image=cv2.copyMakeBorder(im,80,0,0,100,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image,person, (2,60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255),2)\n",
    "    cv2.imshow(name,expanded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomImage(path):\n",
    "    folders=list(filter(lambda x: os.path.isdir(os.path.join(path,x)),os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    print(\"Class - \"+person_dict_n[str(path_class)])\n",
    "    file_path = path+path_class\n",
    "    file_names=[f for f in listdir (file_path) if isfile(join (file_path , f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    return cv2.imread(file_path+'/'+image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class - yash\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_im = getRandomImage(\"dataset/test/\")\n",
    "input_original=input_im.copy()\n",
    "input_original = cv2.resize(input_original,None , fx=0.5,fy=0.5,interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "input_im = cv2.resize(input_im,(224,224),interpolation=cv2.INTER_LINEAR)\n",
    "input_im = input_im / 255.\n",
    "input_im = input_im.reshape(1,224,224,3)\n",
    "\n",
    "res = np.argmax(classifier.predict(input_im, 1 , verbose = 0), axis=1)\n",
    "\n",
    "draw_test('Prediction',res,input_original)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
